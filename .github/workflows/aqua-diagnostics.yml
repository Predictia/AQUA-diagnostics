name: AQUA-diagnostics tests
# This workflow uses pytest-xdist to run tests in parallel across 4 cores

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:
  schedule:
    - cron: "0 3 * * 1" # run every Monday night at 3AM UTC

permissions:
  contents: read

defaults:
  run:
    # NOTE: We must take care with the shell value here. We have steps
    #       with mutiline YAML strings.
    #       These get converted into shell scripts.
    #       If execute as ``bash -l fail_first_command.sh``
    #       then it would fail the first command but run the second
    #       command. The default value in GitHub is ``--noprofile --norc -eo pipefail``.
    #       https://docs.github.com/en/actions/using-workflows/workflow-syntax-for-github-actions#jobsjob_idstepsshell
    #       We removed the noprofile and norc so we have pytest set by Conda in our PATH.
    # NOTE2: login should be kept becuse the action micromamba/setup-micromamba
    #        writes the environment activation script in the .bashrc file.
    shell: bash -l -eo pipefail {0}
jobs:
  aqua_diagnostics-test:
    # NOTE: this option can be uncommented to run the job only when
    #       a pull request contains a label with the name "Run Tests" or "Ready to Merge" or if run by Dependabot.
    if: >
      contains(github.event.pull_request.labels.*.name, 'run tests') ||
      contains(github.event.pull_request.labels.*.name, 'ready to merge') ||
      github.actor == 'dependabot[bot]' ||
      github.event_name == 'workflow_dispatch' ||
      github.event_name == 'schedule'
    env:
      DEBIAN_FRONTEND: noninteractive
      GRID_DEFINITION_PATH: /tmp
      # This is needed for the FDB tests.
      FDB5_CONFIG_FILE: /app/etc/fdb/config.yaml
      # github token: this is a personal access token (PAT) with read access to the repository.
      # It is used to download the AQUA catalog and is available also to dependabot.
      GITHUB_TOKEN: ${{ secrets.AQUA_GITHUB_PAT }}
    
    # This job runs on a Ubuntu machine with 4 cores 
    # see https://docs.github.com/en/actions/reference/runners/github-hosted-runners.
    runs-on: ubuntu-latest
    # This option applies to the individual job of a matrix.
    continue-on-error: false
    container:
      # To run FDB tests we need to use a Docker image with FDB installed.
      image: ghcr.io/destine-climate-dt/ubuntu24.04-fdb5.17.3-eccodes2.41.0-aqua-diagnostics:aqua-diagnostics-container
      options: --user root
    permissions:
      contents: read
      issues: write
      pull-requests: write
    strategy:
      # This option applies to all jobs in the matrix.
      # If true, the entire matrix will stop running when one of the jobs fails.
      fail-fast: false
      matrix:
        python-version: ["3.12"]
    steps:
      # checkout the AQUA-diagnostics code, the Climate-DT-catalog and the aqua-dvc repository
      - name: Checkout code
        uses: actions/checkout@v5
        with:
          path: AQUA-diagnostics
      - name: Checkout Climate-DT-catalog repository
        uses: actions/checkout@v5
        with:
          repository: DestinE-Climate-DT/Climate-DT-catalog
          path: Climate-DT-catalog
          token: ${{ secrets.AQUA_GITHUB_PAT }}
      - name: Checkout DVC repo
        uses: actions/checkout@v5
        with:
          repository: DestinE-Climate-DT/aqua-dvc
          path: aqua-dvc
          token: ${{ secrets.AQUA_DVC_PAT }}
      # The FDB docker image does not have git and other dependencies
      - name: Install GH Actions dependencies
        run: |
          # Ubuntu dependencies (Git, curl, ...)
          apt-get update && \
            apt-get install -y \
              curl \
              git \
              gpg
          # gpg is for codecov
      - name: Set up Micromamba
        uses: mamba-org/setup-micromamba@v2
        with:
          micromamba-version: 'latest'
          environment-file: AQUA-diagnostics/environment.yml
          environment-name: aqua-diagnostics
          cache-downloads: true
          cache-environment: false
          condarc: |
            channels:
              - conda-forge
          create-args: >-
            python=${{ matrix.python-version }}
      - name: List Conda packages
        run: |
          # Save the conda packages visible in the current environment to a file for debugging
          micromamba list -n aqua-diagnostics > AQUA-diagnostics/micromamba_list.txt || conda list > AQUA-diagnostics/micromamba_list.txt || true
          echo "Saved micromamba list to AQUA-diagnostics/micromamba_list.txt"
      - name: List Pip packages
        run: |
          # Save the pip packages visible in the current environment to a file for debugging
          python -m pip freeze > AQUA-diagnostics/pip_freeze.txt || pip freeze > AQUA-diagnostics/pip_freeze.txt || true
          echo "Saved pip freeze to AQUA-diagnostics/pip_freeze.txt"
      - name: Install Flake8
        run: |
          python -m pip install flake8
      - name: Compile code to check for syntax errors
        run: |
          cd AQUA-diagnostics
          python -W error -m compileall -f -q src
      - name: Lint with flake8
        run: |
          cd AQUA-diagnostics
          # stop the build if there are Python syntax errors or undefined names
          flake8 . --count --select=E9,F63,F7,F82,B,B9 --show-source --statistics
          # exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide
          flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics --exclude=__init__.py
      - name: Install DVC s3
        run: |
          python -m pip install "dvc[s3]"
      - name: Configure AWS credentials for DVC
        run: |
          mkdir -p $HOME/.aws
          cat >$HOME/.aws/credentials << EOF
          [default]
          aws_access_key_id = ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws_secret_access_key = ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          EOF
      - name: Pull DVC data and move to the right folder
        run: |
          cd aqua-dvc
          git checkout main
          dvc pull -R data-tests/*
          mkdir -p ../AQUA-test/AQUA_tests
          # Move the entire data-tests folder contents into AQUA_tests
          mv data-tests/* ../AQUA-diagnostics/AQUA_tests
      - name: Set up FDB
        run: |
          export
          # FDB for GSV
          rm -rf /app/
          ls -l *
          cp -r ./AQUA-diagnostics/AQUA_tests/fdb/ /app/
          cat /app/etc/fdb/config.yaml
          mkdir -pv /app/localroot/
          cd /app/
          fdb-write sample_test_data.grib
          fdb-write sample_test_data_d1.grib
          ls -l /app/localroot
          # The fdb-list command is a good smoke test.
          fdb-list class=ea,expver=0001
          # But the fdb-read using a request is more like what pyfdb does.
          # When it fails, the test.grib file will have 0 (zero) messages.
          fdb-read sample_test_data_fdb_request test.grib
          grib_dump test.grib
      - name: Set up AQUA-diagnostics
        run: |
          # Initialize the AQUA catalog in the $HOME/.aqua folder
          aqua -vv install github
          # Add the climatedt-phase1 catalog (for polytope testing)
          aqua -vv add climatedt-phase1
          # Add the ci/cd catalog
          aqua -vv add ci
          # Double check the catalogs
          aqua -vv list
          # add default paths to the config
          cat >> $HOME/.aqua/config-aqua.yaml <<EOF
          paths:
              grids: ./AQUA_tests/grids
              weights: ./AQUA_tests/weights
              areas: ./AQUA_tests/weights
          EOF
          # Set up polytope secrets
          cat >$HOME/.polytopeapirc << EOF
          {
            "user_key": "${{ secrets.POLYTOPE_KEY }}"
          }
          EOF
      - name: Run tests (4 parallel cores with xdist groups)
        run: |
          cd AQUA-diagnostics
          
          echo "=========================================="
          echo "Running all tests with pytest-xdist (4 cores)"
          echo "Tests with xdist_group markers will run on dedicated workers"
          echo "=========================================="
          
          # produce xml reports and junit to be used by codecov. Please notice other options are set in pyproject.toml

          pytest -n 4 \
            --max-worker-restart=3 \
            --cov \
            --junitxml=junit.xml \
            -o junit_family=legacy \
            -m "diagnostics" \
            2>&1 | tee aqua-diagnostics_pytest.log
          
          TEST_EXIT=$?
          
          # Generate coverage reports
          coverage xml
          coverage report
          
          # Check if tests passed
          if [ $TEST_EXIT -ne 0 ]; then
            echo "Tests failed with exit code: $TEST_EXIT"
            exit 1
          fi
      - name: Upload coverage reports to Codecov
        if: ${{ github.actor != 'dependabot[bot]' && !cancelled() }}
        uses: codecov/codecov-action@v5
        with:
          files: AQUA-diagnostics/coverage.xml
          token: ${{ secrets.CODECOV_TOKEN }}
      - name: Upload test results to Codecov
        if: ${{ github.actor != 'dependabot[bot]' && !cancelled() }}
        uses: codecov/test-results-action@v1
        with:
          token: ${{ secrets.CODECOV_TOKEN }}
          files: AQUA-diagnostics/junit.xml
      
      - name: Upload test artifacts (coverage, pytest cache, logs)
        if: ${{ !cancelled() }}
        uses: actions/upload-artifact@v4
        with:
          # Upload common test artifacts to help debugging failed runs
          name: aqua-diagnostics-test-artifacts
          path: |
            AQUA-diagnostics/coverage.xml
            AQUA-diagnostics/.pytest_cache
            AQUA-diagnostics/aqua-diagnostics_pytest.log
            AQUA-diagnostics/micromamba_list.txt
            AQUA-diagnostics/pip_freeze.txt
          # Don't fail the job if some files are missing
          if-no-files-found: ignore